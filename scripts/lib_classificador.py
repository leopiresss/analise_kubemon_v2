import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as pl
from matplotlib.ticker import FuncFormatter
from matplotlib.cm import get_cmap
from itertools import cycle
import itertools
import urllib

from sklearn.datasets import make_classification, load_digits, load_breast_cancer, load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.metrics import roc_curve, auc, confusion_matrix
from sklearn import manifold, datasets
from sklearn.manifold import TSNE

from deslib.util.datasets import make_P2
import xgboost as xgb

from warnings import simplefilter
simplefilter(action='ignore', category=FutureWarning)

class UtilClassificadores:
    """
    Classe utilit√°ria para gerenciar e avaliar classificadores de machine learning
    
    Esta classe facilita a cria√ß√£o, treinamento, avalia√ß√£o e visualiza√ß√£o de m√∫ltiplos
    classificadores de forma padronizada e eficiente.
    
    Atributos:
        TITLES: Lista de classificadores dispon√≠veis
        titles: Lista de classificadores selecionados
        methods: Lista de objetos classificadores instanciados
        valid_titles: Lista de classificadores v√°lidos ap√≥s cria√ß√£o
        trained_models: Dicion√°rio com modelos treinados e seus status
    """
    
    TITLES = ['DecisionTree', 'KNN', 'KNN_W', 'NaiveBayes', 'MLP', 'RF', 'XGBoost', 'ADA', 'Bag']
        
    def __init__(self, titles:list = None, arquivo:str = None):
        """
        Inicializa a classe UtilClassificadores
        
        Par√¢metros:
        - titles: lista de nomes dos classificadores desejados (opcional)
        - arquivo: caminho para arquivo de classificadores salvos (opcional)
        """
        self.titles = titles if titles is not None else self.TITLES
        self.titles = [title for title in self.titles if title in self.TITLES]
        self.methods = []
        self.valid_titles = []
        self.trained_models = {}
        
        if arquivo and os.path.isfile(arquivo):
            print(f"Carregando classificadores do arquivo: {arquivo}")
            self.load_classifiers_from_file(arquivo)
        else:
            print(f"Criando classificadores: {self.titles}")
            self.methods, self.valid_titles = self.create_classifiers(self.titles)


    def load_classifiers_from_file(self, arquivo:str):
        """
        Carrega classificadores de um arquivo
        
        Par√¢metros:
        - arquivo: caminho do arquivo contendo os classificadores
        """
        # TODO: Implementar carregamento de classificadores
        pass

    def save_classifiers_to_file(self, arquivo:str):
        """
        Salva classificadores em um arquivo
        
        Par√¢metros:
        - arquivo: caminho do arquivo para salvar os classificadores
        """
        # TODO: Implementar salvamento de classificadores
        pass

    def get_titles_trained_models(self):
        """
        Retorna os t√≠tulos dos modelos treinados
        
        Retorna:
        - list: lista com t√≠tulos dos modelos v√°lidos
        """
        return self.valid_titles
    
    def get_trained_models(self):
        """
        Retorna o dicion√°rio com todos os modelos treinados
        
        Retorna:
        - dict: dicion√°rio com modelos treinados e seus status
        """
        return self.trained_models
    
    def get_successful_models(self):
        """
        Retorna apenas os modelos treinados com sucesso
        
        Retorna:
        - dict: dicion√°rio filtrado com modelos bem-sucedidos
        """
        return {k: v for k, v in self.trained_models.items() if v['status'] == 'success'}
    
    def __repr__(self):
        """
        Representa√ß√£o textual da classe
        
        Retorna:
        - str: representa√ß√£o formatada da classe
        """
        trained_count = len(self.trained_models)
        successful_count = len(self.get_successful_models())
        return f"UtilClassificadores(modelos={len(self.valid_titles)}, treinados={trained_count}, sucesso={successful_count})"

    def create_classifiers(self, titles=None):
        """
        Cria e configura classificadores de machine learning
        
        Par√¢metros:
        - titles: lista de nomes dos classificadores desejados (opcional)
                Se None, retorna todos os classificadores dispon√≠veis
        
        Retorna:
        - tuple (titles, methods): listas com nomes e objetos dos classificadores
        """
        ['DecisionTree', 'RF', 'KNN', 'NaiveBayes', 'XGBoost', 'ADA']
        # Definir todos os classificadores dispon√≠veis
        available_classifiers = {
            'DecisionTree': DecisionTreeClassifier(criterion='entropy'),
            'KNN': KNeighborsClassifier(n_neighbors=3),
            'KNN_W': KNeighborsClassifier(n_neighbors=3, weights='distance'),
            'NaiveBayes': GaussianNB(var_smoothing=1e-09),
            'MLP': MLPClassifier(solver='lbfgs', early_stopping=True, hidden_layer_sizes=(32), 
                            activation='logistic', batch_size=100, max_iter=10000, 
                            learning_rate_init=0.001, momentum=0.8, random_state=46),
            'RF': RandomForestClassifier(n_estimators=100, random_state=0),
            'XGBoost': xgb.XGBClassifier(objective="binary:logistic", random_state=42),
            'ADA': AdaBoostClassifier(n_estimators=100, random_state=0),
            'Bag': BaggingClassifier(estimator=KNeighborsClassifier(n_neighbors=3), n_estimators=100, random_state=0)
        }
        
        # Configurar SVM com GridSearch
        svm_parameters = [
            {'C': [0.1, 0.5, 1, 10, 100, 500, 1000], 'kernel': ['poly']},
            {'C': [0.1, 0.5, 1, 10, 100, 500, 1000], 'gamma': [0.1, 0.001, 0.0001, 0.00001], 'kernel': ['rbf']},
        ]
        svm_base = SVC(gamma='scale', probability=True, random_state=42)
        available_classifiers['SVM'] = GridSearchCV(svm_base, svm_parameters, scoring='accuracy', cv=10)
        
        # Configurar ensemble voting
        dt_for_voting = DecisionTreeClassifier(criterion='entropy')
        nb_for_voting = GaussianNB(var_smoothing=1e-09)
        knn_for_voting = KNeighborsClassifier(n_neighbors=3)
        
        available_classifiers['DT+NB+KNN'] = VotingClassifier(
            estimators=[('DecisionTree', dt_for_voting), ('NaiveBayes', nb_for_voting), ('knn', knn_for_voting)], 
            voting='soft'
        )
        
        # Se titles n√£o foi especificado, usar configura√ß√£o padr√£o
        if titles is None:
            titles = ['DecisionTree', 'RF', 'KNN', 'NaiveBayes', 'XGBoost', 'ADA']
        
        # Validar se todos os classificadores solicitados existem
        methods = []
        valid_titles = []
        
        print(f"üîß Configurando classificadores solicitados...")
        
        for title in titles:
            if title in available_classifiers:
                methods.append(available_classifiers[title])
                valid_titles.append(title)
                print(f"   ‚úÖ {title}: Configurado")
            else:
                print(f"   ‚ùå {title}: Classificador n√£o encontrado")
                print(f"      Dispon√≠veis: {list(available_classifiers.keys())}")
        
        print(f"\nüìä Resumo:")
        print(f"   ‚Ä¢ Classificadores solicitados: {len(titles)}")
        print(f"   ‚Ä¢ Classificadores configurados: {len(valid_titles)}")
        print(f"   ‚Ä¢ Lista final: {valid_titles}")
        
        return methods, valid_titles


    def train_classifiers(self, X_train=None, y_train=None, verbose=True):
        """
        Treina classificadores de machine learning
        
        Par√¢metros:
        - X_train: dados de treino (features)
        - y_train: r√≥tulos de treino (target)
        - verbose: se True, exibe progresso detalhado
        
        Retorna:
        - dict: dicion√°rio com modelos treinados e status
        """
        
        methods = self.methods
        titles = self.valid_titles
        
        if verbose:
            print("üîß TREINANDO CLASSIFICADORES")
            print("=" * 50)
            print(f"üìä Configura√ß√£o:")
            print(f"   ‚Ä¢ N√∫mero de classificadores: {len(methods)}")
            print(f"   ‚Ä¢ Shape treino: {X_train.shape}")
            print(f"   ‚Ä¢ Classes √∫nicas: {len(np.unique(y_train))}")
        
        # Dicion√°rio para armazenar os modelos treinados
        trained_models = {}

        # Treinar cada classificador
        for i, (method, name) in enumerate(zip(methods, titles)):
            print("Method:", method)
            if verbose:
                print(f"\nüîç [{i+1}/{len(methods)}] Treinando: {name}")
            
            try:
                # Treinar o modelo
                if verbose:
                    print(f"   üéØ Treinando modelo...")
                
                method.fit(X_train, y_train)
                
                trained_models[name] = {
                    'model': method,
                    'status': 'success'
                }
                
                if verbose:
                    print(f"   ‚úÖ {name}: Treinamento conclu√≠do com sucesso")
                    
            except Exception as e:
                if verbose:
                    print(f"   ‚ùå Erro ao treinar {name}: {str(e)}")
                
                trained_models[name] = {
                    'model': method,
                    'status': 'error',
                    'error': str(e)
                }
        
        if verbose:
            successful_models = {k: v for k, v in trained_models.items() if v['status'] == 'success'}
            error_models = {k: v for k, v in trained_models.items() if v['status'] == 'error'}
            
            print(f"\nüìà RESUMO DO TREINAMENTO:")
            print("=" * 40)
            print(f"‚úÖ Modelos treinados com sucesso: {len(successful_models)}")
            if error_models:
                print(f"‚ùå Modelos com erro: {len(error_models)}")
                for name, data in error_models.items():
                    print(f"   ‚Ä¢ {name}: {data['error']}")
        
        self.trained_models = trained_models
        return trained_models


    def evaluate_classifiers(self, X_test_scaled, y_test, verbose=True):
        """
        Avalia classificadores treinados de machine learning
        
        Par√¢metros:
        - X_test_scaled: dados de teste (features) 
        - y_test: r√≥tulos de teste (target)
        - verbose: se True, exibe progresso detalhado
        
        Retorna:
        - tuple: (results dict, scores list) com m√©tricas de acur√°cia por classificador
        """
        
        trained_models = self.trained_models
        titles = self.valid_titles
        
        if verbose:
            print("üìä AVALIANDO CLASSIFICADORES")
            print("=" * 50)
            print(f"üéØ Configura√ß√£o:")
            print(f"   ‚Ä¢ N√∫mero de modelos: {len(trained_models)}")
            print(f"   ‚Ä¢ Shape teste: {X_test_scaled.shape}")
            print(f"   ‚Ä¢ Classes √∫nicas: {len(np.unique(y_test))}")
        
        # Dicion√°rio para armazenar os resultados
        results = {}
        scores = []
        
        # Avaliar cada classificador
        for i, name in enumerate(titles):
            if name in trained_models:
                model_info = trained_models[name]
                
                if verbose:
                    print(f"\nüìä [{i+1}/{len(titles)}] Avaliando: {name}")
                
                if model_info['status'] == 'success':
                    try:
                        # Calcular acur√°cia no conjunto de teste
                        model = model_info['model']
                        accuracy = model.score(X_test_scaled, y_test)
                        scores.append(accuracy)
                        results[name] = {
                            'accuracy': accuracy,
                            'model': model,
                            'status': 'success'
                        }
                        
                        if verbose:
                            print(f"   ‚úÖ Classification accuracy {name} = {accuracy:.4f}")
                            
                    except Exception as e:
                        if verbose:
                            print(f"   ‚ùå Erro ao avaliar {name}: {str(e)}")
                        
                        scores.append(None)
                        results[name] = {
                            'accuracy': None,
                            'model': model_info['model'],
                            'status': 'error',
                            'error': str(e)
                        }
                else:
                    # Modelo n√£o foi treinado com sucesso
                    if verbose:
                        print(f"   ‚ö†Ô∏è {name}: Modelo n√£o foi treinado (pulando avalia√ß√£o)")
                    
                    scores.append(None)
                    results[name] = {
                        'accuracy': None,
                        'model': model_info['model'],
                        'status': 'not_trained',
                        'error': model_info.get('error', 'Modelo n√£o foi treinado com sucesso')
                    }
        
        if verbose:
            print(f"\nüìà RESUMO DOS RESULTADOS:")
            print("=" * 50)
            
            # Filtrar apenas sucessos
            successful_results = {k: v for k, v in results.items() if v['status'] == 'success'}
            
            if successful_results:
                # Ordenar por acur√°cia
                sorted_results = sorted(successful_results.items(), 
                                    key=lambda x: x[1]['accuracy'], 
                                    reverse=True)
                
                print(f"‚úÖ Modelos avaliados com sucesso: {len(successful_results)}")
                print(f"üìä Ranking por acur√°cia:")
                
                for rank, (name, data) in enumerate(sorted_results, 1):
                    medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank:2d}."
                    print(f"   {medal} {name:15s}: {data['accuracy']:7.4f}")
                
                # Estat√≠sticas resumidas
                accuracies = [data['accuracy'] for data in successful_results.values()]
                print(f"\nüìä Estat√≠sticas:")
                print(f"   ‚Ä¢ Melhor acur√°cia: {max(accuracies):.4f}")
                print(f"   ‚Ä¢ Pior acur√°cia: {min(accuracies):.4f}")
                print(f"   ‚Ä¢ Acur√°cia m√©dia: {np.mean(accuracies):.4f}")
                print(f"   ‚Ä¢ Desvio padr√£o: {np.std(accuracies):.4f}")
            
            # Relat√≥rio de erros
            error_results = {k: v for k, v in results.items() if v['status'] in ['error', 'not_trained']}
            if error_results:
                print(f"\n‚ùå Modelos com problemas: {len(error_results)}")
                for name, data in error_results.items():
                    print(f"   ‚Ä¢ {name}: {data['error']}")
        
        return results, scores

    def get_methods_trained_models(self, trained_models=None):
        """
        Extrai apenas os m√©todos/modelos treinados com sucesso
        
        Par√¢metros:
        - trained_models: dicion√°rio com modelos treinados (opcional, usa self.trained_models se None)
        
        Retorna:
        - list: lista de modelos treinados com sucesso
        """
        if trained_models is None:
            trained_models = self.trained_models
            
        methods = []
        valid_titles = []
        
        for title in trained_models.keys():
            if title in trained_models and trained_models[title]['status'] == 'success':
                methods.append(trained_models[title]['model'])
                valid_titles.append(title)
                
        return methods, valid_titles

    def generate_model_metrics_dataset(self, X_test_scaled, y_test, dataset_name, 
                                    save_to_csv=True, save_dir='../dataset', 
                                    display_results=True, verbose=True):
        """
        Gera dataset completo com m√©tricas de avalia√ß√£o para m√∫ltiplos modelos de ML
        
        Par√¢metros:
        - X_test_scaled: dados de teste (features)
        - y_test: r√≥tulos de teste (target)
        - dataset_name: nome do dataset para identifica√ß√£o
        - save_to_csv: se True, salva o dataset em arquivo CSV
        - save_dir: diret√≥rio para salvar o arquivo CSV
        - display_results: se True, exibe tabelas e estat√≠sticas detalhadas
        - verbose: se True, exibe progresso detalhado
        
        Retorna:
        - pandas.DataFrame: DataFrame com m√©tricas de todos os modelos
        """
        
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
        import pandas as pd
        import os
        from datetime import datetime
        import numpy as np
        
        methods_trained_models = self.trained_models
        titles = self.valid_titles
        
        if verbose:
            print("üìä GERANDO DATASET COM M√âTRICAS DE AVALIA√á√ÉO DOS MODELOS")
            print("=" * 70)
            print(f"üéØ Configura√ß√£o:")
            print(f"   ‚Ä¢ N√∫mero de modelos: {len(methods_trained_models)}")
            print(f"   ‚Ä¢ Dataset: {dataset_name}")
            print(f"   ‚Ä¢ Shape dados teste: {X_test_scaled.shape}")
            print(f"   ‚Ä¢ Classes √∫nicas: {len(np.unique(y_test))}")
            print(f"   ‚Ä¢ Salvar CSV: {'Sim' if save_to_csv else 'N√£o'}")
        
        # Lista para armazenar as m√©tricas
        metrics_data = []
        
        # Filtrar apenas modelos treinados com sucesso      
        methods, valid_titles = self.get_methods_trained_models(methods_trained_models)
        print("Titles valids:", valid_titles)
        # Iterar por todos os modelos treinados
        for i, (clf, title) in enumerate(zip(methods, titles)):
            if verbose:
                print(f"\nüîç [{i+1}/{len(methods)}] Avaliando modelo: {title}")
            
            try:
                # Fazer predi√ß√µes no conjunto de teste
                y_pred = clf.predict(X_test_scaled)
                
                # Calcular m√©tricas principais
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

                # Adicionar dados √† lista
                metrics_data.append({
                    'modelo': title,
                    'acuracia': accuracy,
                    'precisao': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'dataset': dataset_name,
                    'data_avaliacao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'num_amostras_teste': len(y_test),
                    'num_classes': len(np.unique(y_test)),
                    'status': 'success'
                })
                
                if verbose:
                    print(f"   ‚úÖ {title}: Acur√°cia={accuracy:.4f}, Precis√£o={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}")
                    
            except Exception as e:
                if verbose:
                    print(f"   ‚ùå Erro ao avaliar {title}: {e}")
                
                # Adicionar dados com erro
                metrics_data.append({
                    'modelo': title,
                    'acuracia': None,
                    'precisao': None,
                    'recall': None,
                    'f1_score': None,
                    'dataset': dataset_name,
                    'data_avaliacao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'num_amostras_teste': len(y_test),
                    'num_classes': len(np.unique(y_test)),
                    'status': 'error',
                    'erro': str(e)
                })

        # Criar DataFrame com as m√©tricas
        metrics_df = pd.DataFrame(metrics_data)
        
        if display_results and verbose:
            # Exibir o dataset
            print(f"\nüìã DATASET DE M√âTRICAS GERADO:")
            print("=" * 80)
            display_columns = ['modelo', 'acuracia', 'precisao', 'recall', 'f1_score', 'status']
            print(metrics_df[display_columns].to_string(index=False, float_format='%.4f'))
            
            # Estat√≠sticas resumidas
            print(f"\nüìà ESTAT√çSTICAS RESUMIDAS:")
            print("=" * 50)
            
            # Filtrar apenas modelos com sucesso
            valid_metrics = metrics_df[metrics_df['acuracia'].notna()]
            if len(valid_metrics) > 0:
                print(f"‚úÖ Modelos avaliados com sucesso: {len(valid_metrics)}")
                print(f"‚ùå Modelos com erro: {len(metrics_df) - len(valid_metrics)}")
                
                # Melhores resultados por m√©trica
                print(f"\nüèÜ MELHORES RESULTADOS:")
                print(f"   ‚Ä¢ Melhor acur√°cia: {valid_metrics['acuracia'].max():.4f} ({valid_metrics.loc[valid_metrics['acuracia'].idxmax(), 'modelo']})")
                print(f"   ‚Ä¢ Melhor precis√£o: {valid_metrics['precisao'].max():.4f} ({valid_metrics.loc[valid_metrics['precisao'].idxmax(), 'modelo']})")
                print(f"   ‚Ä¢ Melhor recall: {valid_metrics['recall'].max():.4f} ({valid_metrics.loc[valid_metrics['recall'].idxmax(), 'modelo']})")
                print(f"   ‚Ä¢ Melhor F1-Score: {valid_metrics['f1_score'].max():.4f} ({valid_metrics.loc[valid_metrics['f1_score'].idxmax(), 'modelo']})")
                
                # M√©dias das m√©tricas
                print(f"\nüìä M√âDIAS DAS M√âTRICAS:")
                print(f"   ‚Ä¢ Acur√°cia m√©dia: {valid_metrics['acuracia'].mean():.4f} (¬±{valid_metrics['acuracia'].std():.4f})")
                print(f"   ‚Ä¢ Precis√£o m√©dia: {valid_metrics['precisao'].mean():.4f} (¬±{valid_metrics['precisao'].std():.4f})")
                print(f"   ‚Ä¢ Recall m√©dio: {valid_metrics['recall'].mean():.4f} (¬±{valid_metrics['recall'].std():.4f})")
                print(f"   ‚Ä¢ F1-Score m√©dio: {valid_metrics['f1_score'].mean():.4f} (¬±{valid_metrics['f1_score'].std():.4f})")
            else:
                print("‚ùå Nenhum modelo foi avaliado com sucesso!")
        
        # Salvar o dataset em arquivo CSV
        if save_to_csv:
            if verbose:
                print(f"\nüíæ SALVANDO DATASET:")
                print("-" * 30)
            
            try:
                # Criar diret√≥rio se n√£o existir
                os.makedirs(save_dir, exist_ok=True)
                
                # Gerar nome do arquivo com timestamp
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                metrics_filename = f'metricas_modelos_{dataset_name}_{timestamp}.csv'
                metrics_filepath = os.path.join(save_dir, metrics_filename)
                
                # Salvar arquivo
                metrics_df.to_csv(metrics_filepath, index=False, encoding='utf-8')
                
                if verbose:
                    print(f"   ‚úÖ Dataset salvo em: {metrics_filepath}")
                    
                    # Verificar o tamanho do arquivo
                    file_size = os.path.getsize(metrics_filepath)
                    print(f"   üìÅ Tamanho do arquivo: {file_size} bytes ({file_size/1024:.2f} KB)")
                    
            except Exception as e:
                if verbose:
                    print(f"   ‚ùå Erro ao salvar dataset: {e}")
        
        if display_results and verbose:
            # Ranking dos modelos por acur√°cia
            print(f"\nüèÜ RANKING DOS MODELOS (por acur√°cia):")
            print("=" * 70)
            
            valid_models = metrics_df[metrics_df['acuracia'].notna()].copy()
            if len(valid_models) > 0:
                valid_models_sorted = valid_models.sort_values('acuracia', ascending=False).reset_index(drop=True)
                
                for i, row in valid_models_sorted.iterrows():
                    medal = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1:2d}."
                    print(f"{medal} {row['modelo']:15s}: {row['acuracia']:6.4f} " +
                        f"(Precis√£o: {row['precisao']:6.4f}, Recall: {row['recall']:6.4f}, F1: {row['f1_score']:6.4f})")
            
            # Listar erros se houver
            error_models = metrics_df[metrics_df['status'] == 'error']
            if len(error_models) > 0:
                print(f"\n‚ùå MODELOS COM ERRO:")
                print("-" * 40)
                for _, row in error_models.iterrows():
                    erro_msg = row.get('erro', 'Erro n√£o especificado')
                    print(f"   ‚Ä¢ {row['modelo']}: {erro_msg}")
            
            print(f"\nüéâ An√°lise completa! Dataset gerado com {len(metrics_df)} modelos.")
            print(f"   ‚Ä¢ Sucessos: {len(valid_models)} modelos")
            print(f"   ‚Ä¢ Erros: {len(error_models)} modelos")
            print(f"üí° O DataFrame foi retornado para an√°lises adicionais.")
        
        return metrics_df



    def generate_confusion_matrices(self, X_test_scaled, y_test, figsize_per_model=(7, 4), cols=2, save_path=None, verbose=True):
        """
        Gera matrizes de confus√£o para m√∫ltiplos classificadores
        
        Par√¢metros:
        - X_test_scaled: dados de teste (features) j√° normalizados
        - y_test: r√≥tulos de teste (target)
        - figsize_per_model: tupla com tamanho base de cada subplot (largura, altura)
        - cols: n√∫mero de colunas no layout de subplots (padr√£o: 2)
        - save_path: caminho para salvar a figura (opcional)
        - verbose: se True, exibe progresso detalhado
        
        Retorna:
        - dict: dicion√°rio com m√©tricas de acur√°cia por classificador
        """
        
        from sklearn.metrics import confusion_matrix
        import matplotlib.pyplot as plt
        import seaborn as sns
        import pandas as pd
        import numpy as np

        trained_models = self.trained_models
        titles = self.valid_titles
        
        # Filtrar apenas modelos treinados com sucesso
        methods, valid_titles = self.get_methods_trained_models(trained_models)
        titles = valid_titles
        
        print("=====", titles)
        if verbose:
            print("üéØ GERANDO MATRIZES DE CONFUS√ÉO")
            print("=" * 50)
            print(f"üìä Configura√ß√£o:")
            print(f"   ‚Ä¢ N√∫mero de modelos: {len(methods)}")
            print(f"   ‚Ä¢ Layout: {cols} colunas")
            print(f"   ‚Ä¢ Shape dados teste: {X_test_scaled.shape}")
            print(f"   ‚Ä¢ Classes √∫nicas: {len(np.unique(y_test))}")
        
        # Calcular n√∫mero de linhas necess√°rias
        n_models = len(methods)
        n_rows = (n_models + cols - 1) // cols  # Arredonda para cima
        
        # Criar subplots
        fig_width = figsize_per_model[0] * cols
        fig_height = figsize_per_model[1] * n_rows
        
        fig, axes = plt.subplots(n_rows, cols, figsize=(fig_width, fig_height))
        plt.subplots_adjust(wspace=0.4, hspace=0.4)
        
        # Garantir que axes seja sempre um array 2D
        if n_rows == 1 and cols == 1:
            axes = np.array([[axes]])
        elif n_rows == 1:
            axes = axes.reshape(1, -1)
        elif cols == 1:
            axes = axes.reshape(-1, 1)
        
        # Dicion√°rio para armazenar resultados
        results = {}
        
        # Iterar pelos modelos e plotar matrizes de confus√£o
        for i, (clf, title) in enumerate(zip(methods, titles)):
            # Calcular posi√ß√£o na grade
            print(">>>>", clf, title)
            row = i // cols
            col = i % cols
            ax = axes[row, col]
            
            if verbose:
                print(f"\nüìä [{i+1}/{n_models}] Processando: {title}")
            
            try:
                # Fazer predi√ß√µes
                y_predicted = clf.predict(X_test_scaled)
                
                # Calcular matriz de confus√£o
                cm = confusion_matrix(y_test, y_predicted)
                
                # Calcular acur√°cia
                accuracy = (cm.diagonal().sum() / cm.sum()) * 100
                
                # Criar DataFrame para melhor visualiza√ß√£o
                unique_labels = np.unique(np.concatenate([y_test, y_predicted]))
                df_cm = pd.DataFrame(cm, 
                                index=[f'Real {label}' for label in unique_labels],
                                columns=[f'Pred {label}' for label in unique_labels])
                
                # Plotar usando seaborn heatmap
                sns.heatmap(df_cm, 
                        annot=True, 
                        fmt='d', 
                        cmap='Blues',
                        ax=ax,
                        cbar_kws={'shrink': 0.8})
                
                # Configurar subplot
                ax.set_title(f'Matriz de Confus√£o - {title}', 
                            fontsize=12, fontweight='bold')
                ax.set_xlabel('Classe Predita', fontsize=10)
                ax.set_ylabel('Classe Real', fontsize=10)
                
                # Adicionar texto com acur√°cia
                ax.text(0.02, 0.98, f'Acur√°cia: {accuracy:.2f}%', 
                    transform=ax.transAxes,
                    verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                    fontsize=9)
                
                # Armazenar resultados
                results[title] = {
                    'accuracy': accuracy / 100,  # Normalizar para [0,1]
                    'confusion_matrix': cm,
                    'predictions': y_predicted,
                    'status': 'success'
                }
                
                if verbose:
                    print(f"   ‚úÖ {title}: Acur√°cia = {accuracy:.2f}%")
                    
            except Exception as e:
                if verbose:
                    print(f"   ‚ùå Erro ao processar {title}: {e}")
                
                # Plotar mensagem de erro
                ax.text(0.5, 0.5, f'Erro ao gerar\nmatriz para {title}', 
                    ha='center', va='center',
                    transform=ax.transAxes,
                    fontsize=12, color='red')
                ax.set_title(f'Erro - {title}', fontsize=12, color='red')
                
                # Armazenar erro
                results[title] = {
                    'accuracy': None,
                    'confusion_matrix': None,
                    'predictions': None,
                    'status': 'error',
                    'error': str(e)
                }
        
        # Ocultar subplots extras
        for idx in range(n_models, n_rows * cols):
            row = idx // cols
            col = idx % cols
            axes[row, col].set_visible(False)
        
        # Configurar t√≠tulo geral
        plt.suptitle('Matrizes de Confus√£o - Compara√ß√£o de Modelos', 
                    fontsize=16, fontweight='bold', y=0.98)
        
        plt.tight_layout()
        
        # Salvar figura se solicitado
        if save_path:
            try:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                if verbose:
                    print(f"\nüíæ Figura salva em: {save_path}")
            except Exception as e:
                if verbose:
                    print(f"\n‚ùå Erro ao salvar figura: {e}")
        
        plt.show()
        
        if verbose:
            print(f"\nüìà RESUMO DOS RESULTADOS:")
            print("=" * 50)
            
            # Filtrar sucessos
            successful_results = {k: v for k, v in results.items() if v['status'] == 'success'}
            
            if successful_results:
                # Ranking por acur√°cia
                sorted_results = sorted(successful_results.items(), 
                                    key=lambda x: x[1]['accuracy'], 
                                    reverse=True)
                
                print(f"‚úÖ Matrizes geradas com sucesso: {len(successful_results)}")
                print(f"üìä Ranking por acur√°cia:")
                
                for rank, (name, data) in enumerate(sorted_results, 1):
                    medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank:2d}."
                    accuracy_pct = data['accuracy'] * 100
                    print(f"   {medal} {name:15s}: {accuracy_pct:6.2f}%")
                
                # Estat√≠sticas
                accuracies = [data['accuracy'] * 100 for data in successful_results.values()]
                print(f"\nüìä Estat√≠sticas de Acur√°cia:")
                print(f"   ‚Ä¢ Melhor: {max(accuracies):.2f}%")
                print(f"   ‚Ä¢ Pior: {min(accuracies):.2f}%")
                print(f"   ‚Ä¢ M√©dia: {np.mean(accuracies):.2f}%")
                print(f"   ‚Ä¢ Desvio: {np.std(accuracies):.2f}%")
            
            # Relat√≥rio de erros
            error_results = {k: v for k, v in results.items() if v['status'] == 'error'}
            if error_results:
                print(f"\n‚ùå Modelos com erro: {len(error_results)}")
                for name, data in error_results.items():
                    print(f"   ‚Ä¢ {name}: {data['error']}")
            
            print(f"\nüéâ Layout: {n_rows} linhas √ó {cols} colunas")
            print(f"üìä Total processado: {n_models} modelos")
        
        return results



    def generate_roc_auc_curves(self, X_test_scaled, y_test, nome_dataset='Dataset', verbose=True):
        """
        Gera curvas ROC/AUC para m√∫ltiplos classificadores treinados
        
        Par√¢metros:
        - X_test_scaled: dados de teste (features) j√° normalizados
        - y_test: r√≥tulos de teste (target)
        - nome_dataset: nome do dataset para t√≠tulo dos gr√°ficos
        - verbose: se True, exibe progresso detalhado
        
        Retorna:
        - None
        """
        
        trained_models = self.trained_models
        titles = list(trained_models.keys())

        print("üìà Gerando Curvas ROC/AUC para todos os modelos...")

        # Verificar se √© classifica√ß√£o bin√°ria ou multiclasse
        n_classes = len(np.unique(y_test))
    
        # Configurar cores para os gr√°ficos
        colors = cycle(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])

        # Lista para armazenar os valores AUC
        auc_scores = []

        # Filtrar apenas modelos treinados com sucesso
        methods, valid_titles = self.get_methods_trained_models(trained_models)

        if verbose:
            for title in titles:
                if title not in valid_titles:
                    print(f"   ‚ö†Ô∏è Pulando {title}: {trained_models.get(title, {}).get('error', 'N√£o encontrado')}")

        if n_classes == 2:
            # CLASSIFICA√á√ÉO BIN√ÅRIA - ROC/AUC tradicional
            print("\nüéØ Processando classifica√ß√£o bin√°ria...")
            
            plt.figure(figsize=(12, 8))
            
            for clf, title, color in zip(methods, titles, colors):
                print(f"üìä Processando {title}...")
                
                try:
                    # Obter probabilidades de predi√ß√£o (necess√°rio para ROC)
                    if hasattr(clf, "predict_proba"):
                        y_score = clf.predict_proba(X_test_scaled)[:, 1]
                    elif hasattr(clf, "decision_function"):
                        y_score = clf.decision_function(X_test_scaled)
                    else:
                        print(f"   ‚ö†Ô∏è {title}: Modelo n√£o suporta probabilidades - usando predi√ß√µes bin√°rias")
                        y_score = clf.predict(X_test_scaled)
                    
                    # Calcular curva ROC
                    fpr, tpr, _ = roc_curve(y_test, y_score)
                    roc_auc = auc(fpr, tpr)
                    auc_scores.append(roc_auc)
                    
                    # Plotar curva ROC
                    plt.plot(fpr, tpr, color=color, lw=2, 
                            label=f'{title} (AUC = {roc_auc:.3f})')
                    
                    print(f"   ‚úÖ {title}: AUC = {roc_auc:.4f}")
                    
                except Exception as e:
                    print(f"   ‚ùå Erro ao processar {title}: {e}")
                    auc_scores.append(np.nan)
            
            # Adicionar linha diagonal (classificador aleat√≥rio)
            plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', 
                    label='Classificador Aleat√≥rio (AUC = 0.500)')
            
            # Configurar gr√°fico
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('Taxa de Falso Positivo (1 - Especificidade)', fontsize=12)
            plt.ylabel('Taxa de Verdadeiro Positivo (Sensibilidade)', fontsize=12)
            plt.title(f'Curvas ROC - Compara√ß√£o de Modelos\nDataset: {nome_dataset}', fontsize=14, fontweight='bold')
            plt.legend(loc="lower right", fontsize=10)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.show()


        # Gr√°fico de barras com scores AUC
        print(f"\nüìä Gerando gr√°fico de compara√ß√£o dos scores AUC...")

        plt.figure(figsize=(12, 6))
        valid_indices = [i for i, score in enumerate(auc_scores) if not np.isnan(score)]
        valid_scores = [auc_scores[i] for i in valid_indices]
        valid_titles = [titles[i] for i in valid_indices]

        if valid_scores:
            colors_bar = plt.cm.Set3(np.linspace(0, 1, len(valid_scores)))
            bars = plt.bar(range(len(valid_scores)), valid_scores, color=colors_bar, 
                        edgecolor='black', linewidth=1)
            
            # Adicionar valores no topo das barras
            for i, (bar, score) in enumerate(zip(bars, valid_scores)):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
            
            plt.xlabel('Modelos', fontsize=12)
            plt.ylabel('Score AUC', fontsize=12)
            plt.title(f'Compara√ß√£o de Scores AUC - Dataset: {nome_dataset}', 
                    fontsize=14, fontweight='bold')
            plt.xticks(range(len(valid_scores)), valid_titles, rotation=45, ha='right')
            plt.ylim(0, 1.1)
            plt.grid(True, axis='y', alpha=0.3)
            plt.tight_layout()
            plt.show()

        # Resumo dos resultados
        print(f"\nüèÜ RESUMO DOS SCORES AUC:")
        print("=" * 60)
        auc_data = []
        for title, score in zip(titles, auc_scores):
            if not np.isnan(score):
                auc_data.append({'modelo': title, 'auc_score': score})
                print(f"‚úÖ {title:15s}: {score:.4f}")
            else:
                print(f"‚ùå {title:15s}: ERROR")

        if auc_data:
            # Criar DataFrame com scores AUC
            auc_df = pd.DataFrame(auc_data)
            auc_df_sorted = auc_df.sort_values('auc_score', ascending=False).reset_index(drop=True)
            
            print(f"\nü•á RANKING POR AUC:")
            print("-" * 40)
            for i, row in auc_df_sorted.iterrows():
                medal = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1:2d}."
                print(f"{medal} {row['modelo']:15s}: {row['auc_score']:.4f}")
            
            print(f"\nüìä ESTAT√çSTICAS AUC:")
            print("-" * 30)
            print(f"Melhor AUC: {auc_df['auc_score'].max():.4f} ({auc_df.loc[auc_df['auc_score'].idxmax(), 'modelo']})")
            print(f"AUC M√©dio: {auc_df['auc_score'].mean():.4f}")
            print(f"Desvio Padr√£o: {auc_df['auc_score'].std():.4f}")
            
            # Disponibilizar DataFrame para uso posterior
            globals()['auc_df'] = auc_df
            print(f"\nüí° DataFrame 'auc_df' dispon√≠vel para an√°lises adicionais.")
