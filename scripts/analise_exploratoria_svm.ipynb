{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä An√°lise Explorat√≥ria de Dados - Dataset SVM\n",
    "\n",
    "Este notebook cont√©m uma an√°lise explorat√≥ria completa do dataset `svm_claude.csv`.\n",
    "\n",
    "**Objetivo:** Analisar as caracter√≠sticas do dataset e identificar padr√µes para classifica√ß√£o da vari√°vel `target`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Estat√≠stica\n",
    "from scipy import stats\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "df = pd.read_csv('svm_claude.csv')\n",
    "\n",
    "print(f\"Dataset carregado com sucesso!\")\n",
    "print(f\"Dimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vis√£o Geral dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes b√°sicas\n",
    "print(\"=\" * 80)\n",
    "print(\"INFORMA√á√ïES B√ÅSICAS DO DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas\")\n",
    "print(f\"\\nMem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiras linhas\n",
    "print(\"\\nPrimeiras 5 linhas do dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes detalhadas\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores ausentes\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DE VALORES AUSENTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Ausentes': missing,\n",
    "    'Percentual (%)': missing_pct\n",
    "})\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"\\n‚úÖ Nenhum valor ausente encontrado!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Features com valores ausentes:\")\n",
    "    print(missing_df[missing_df['Valores Ausentes'] > 0].sort_values('Valores Ausentes', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicatas\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nLinhas duplicadas: {duplicates}\")\n",
    "if duplicates == 0:\n",
    "    print(\"‚úÖ Nenhuma linha duplicada encontrada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise da Vari√°vel Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribui√ß√£o do target\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DA VARI√ÅVEL TARGET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nDistribui√ß√£o:\")\n",
    "print(df['target'].value_counts())\n",
    "print(\"\\nPropor√ß√£o:\")\n",
    "print(df['target'].value_counts(normalize=True))\n",
    "\n",
    "# Calcular desbalanceamento\n",
    "counts = df['target'].value_counts()\n",
    "desbalanceamento = counts.max() / counts.min()\n",
    "print(f\"\\nDesbalanceamento: {desbalanceamento:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da distribui√ß√£o do target\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "target_counts = df['target'].value_counts()\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=colors, edgecolor='black', linewidth=2)\n",
    "axes[0].set_title('Distribui√ß√£o da Vari√°vel Target', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Classe', fontsize=12)\n",
    "axes[0].set_ylabel('Frequ√™ncia', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90, explode=[0.05, 0.05],\n",
    "            textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Propor√ß√£o das Classes', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Distribui√ß√£o de iteration por target\n",
    "for target_class in df['target'].unique():\n",
    "    data = df[df['target'] == target_class]['iteration']\n",
    "    axes[2].hist(data, alpha=0.6, label=target_class, bins=20, edgecolor='black')\n",
    "axes[2].set_title('Distribui√ß√£o de Iteration por Target', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Iteration', fontsize=12)\n",
    "axes[2].set_ylabel('Frequ√™ncia', fontsize=12)\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Estat√≠sticas Descritivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas gerais\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTAT√çSTICAS DESCRITIVAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'iteration' in numeric_cols:\n",
    "    numeric_cols.remove('iteration')\n",
    "\n",
    "print(f\"\\nTotal de features num√©ricas: {len(numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Correla√ß√£o com Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar encoding num√©rico para target\n",
    "df['target_encoded'] = (df['target'] == 'interf').astype(int)\n",
    "\n",
    "# Calcular correla√ß√£o com target\n",
    "correlations = df[numeric_cols].corrwith(df['target_encoded']).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 20 FEATURES COM MAIOR CORRELA√á√ÉO COM TARGET\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n",
    "for i, (feature, corr) in enumerate(correlations.head(20).items(), 1):\n",
    "    print(f\"{i:2d}. {feature:50s} {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o das correla√ß√µes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Top 15 correla√ß√µes\n",
    "top_corr = correlations.head(15)\n",
    "colors_corr = ['#FF6B6B' if x > 0.3 else '#4ECDC4' if x > 0.15 else '#95E1D3' for x in top_corr.values]\n",
    "axes[0].barh(range(len(top_corr)), top_corr.values, color=colors_corr, edgecolor='black')\n",
    "axes[0].set_yticks(range(len(top_corr)))\n",
    "axes[0].set_yticklabels([col.replace('mean_', '').replace('_', ' ')[:35] for col in top_corr.index], fontsize=9)\n",
    "axes[0].set_xlabel('Correla√ß√£o Absoluta com Target', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Top 15 Features - Correla√ß√£o com Target', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Distribui√ß√£o das correla√ß√µes\n",
    "axes[1].hist(correlations.values, bins=30, color='#4ECDC4', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(correlations.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'M√©dia: {correlations.mean():.3f}')\n",
    "axes[1].axvline(correlations.median(), color='orange', linestyle='--', linewidth=2, \n",
    "                label=f'Mediana: {correlations.median():.3f}')\n",
    "axes[1].set_xlabel('Correla√ß√£o Absoluta', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequ√™ncia', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Distribui√ß√£o das Correla√ß√µes com Target', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o das top 10 features\n",
    "top_10_features = correlations.head(10).index.tolist()\n",
    "corr_matrix = df[top_10_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correla√ß√£o - Top 10 Features', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lise de Distribui√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas de distribui√ß√£o\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DE DISTRIBUI√á√ïES DAS FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stats_summary = pd.DataFrame({\n",
    "    'mean': df[numeric_cols].mean(),\n",
    "    'std': df[numeric_cols].std(),\n",
    "    'min': df[numeric_cols].min(),\n",
    "    'max': df[numeric_cols].max(),\n",
    "    'skewness': df[numeric_cols].skew(),\n",
    "    'kurtosis': df[numeric_cols].kurtosis(),\n",
    "    'zeros_count': (df[numeric_cols] == 0).sum(),\n",
    "    'zeros_pct': ((df[numeric_cols] == 0).sum() / len(df) * 100)\n",
    "})\n",
    "\n",
    "print(\"\\nFeatures com mais de 80% de zeros:\")\n",
    "high_zeros = stats_summary[stats_summary['zeros_pct'] > 80].sort_values('zeros_pct', ascending=False)\n",
    "print(f\"Total: {len(high_zeros)} features\")\n",
    "print(high_zeros[['zeros_count', 'zeros_pct']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features com distribui√ß√£o mais assim√©trica\n",
    "print(\"\\nFeatures com distribui√ß√£o mais assim√©trica (|skewness| > 5):\")\n",
    "high_skew = stats_summary[stats_summary['skewness'].abs() > 5].sort_values('skewness', ascending=False)\n",
    "print(f\"Total: {len(high_skew)} features\")\n",
    "print(high_skew[['mean', 'std', 'skewness']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o das distribui√ß√µes das top features\n",
    "top_features = correlations.head(6).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    for target_class in df['target'].unique():\n",
    "        data = df[df['target'] == target_class][feature]\n",
    "        axes[idx].hist(data, alpha=0.6, label=target_class, bins=25, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    axes[idx].set_title(f'{feature.replace(\"mean_\", \"\").replace(\"_\", \" \")[:40]}', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Valor', fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequ√™ncia', fontsize=10)\n",
    "    axes[idx].legend(fontsize=9)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots comparativos\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    data_to_plot = [df[df['target'] == target_class][feature].values \n",
    "                    for target_class in df['target'].unique()]\n",
    "    \n",
    "    bp = axes[idx].boxplot(data_to_plot, tick_labels=df['target'].unique(), \n",
    "                           patch_artist=True, showmeans=True)\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    axes[idx].set_title(f'Boxplot: {feature.replace(\"mean_\", \"\").replace(\"_\", \" \")[:35]}', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Valor', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lise por Categorias de M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features por categoria\n",
    "categories = {\n",
    "    'OS CPU': [col for col in df.columns if col.startswith('mean_os_cpu')],\n",
    "    'OS Disk': [col for col in df.columns if col.startswith('mean_os_disk')],\n",
    "    'OS Memory': [col for col in df.columns if col.startswith('mean_os_mem')],\n",
    "    'OS Network': [col for col in df.columns if col.startswith('mean_os_net')],\n",
    "    'Process CPU': [col for col in df.columns if col.startswith('mean_process_cpu')],\n",
    "    'Process Disk': [col for col in df.columns if col.startswith('mean_process_disk')],\n",
    "    'Process Memory': [col for col in df.columns if col.startswith('mean_process_mem')],\n",
    "    'Process Network': [col for col in df.columns if col.startswith('mean_process_net')],\n",
    "    'Container CPU': [col for col in df.columns if col.startswith('mean_container_cpu')],\n",
    "    'Container Disk': [col for col in df.columns if col.startswith('mean_container_disk')],\n",
    "    'Container Memory': [col for col in df.columns if col.startswith('mean_container_mem')],\n",
    "    'Container Network': [col for col in df.columns if col.startswith('mean_container_net')]\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE POR CATEGORIAS DE M√âTRICAS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n",
    "\n",
    "for category, features in categories.items():\n",
    "    print(f\"{category:20s}: {len(features):3d} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top feature por categoria\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP FEATURE POR CATEGORIA (maior correla√ß√£o com target)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n",
    "\n",
    "category_top_features = {}\n",
    "for category, features in categories.items():\n",
    "    if features:\n",
    "        corrs = df[features].corrwith(df['target_encoded']).abs()\n",
    "        if not corrs.empty and corrs.max() > 0:\n",
    "            top_feature = corrs.idxmax()\n",
    "            top_corr = corrs.max()\n",
    "            category_top_features[category] = (top_feature, top_corr)\n",
    "            print(f\"{category:20s}: {top_feature:50s} (corr: {top_corr:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o por categorias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# N√∫mero de features por categoria\n",
    "cat_names = list(categories.keys())\n",
    "cat_counts = [len(categories[cat]) for cat in cat_names]\n",
    "colors = plt.cm.Set3(range(len(cat_names)))\n",
    "axes[0].barh(cat_names, cat_counts, color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('N√∫mero de Features', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Features por Categoria', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(cat_counts):\n",
    "    axes[0].text(v + 0.5, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "# Correla√ß√£o m√©dia por categoria\n",
    "cat_mean_corrs = []\n",
    "for category, features in categories.items():\n",
    "    if features:\n",
    "        mean_corr = df[features].corrwith(df['target_encoded']).abs().mean()\n",
    "        cat_mean_corrs.append((category, mean_corr))\n",
    "\n",
    "cat_mean_corrs.sort(key=lambda x: x[1], reverse=True)\n",
    "cat_names_sorted = [x[0] for x in cat_mean_corrs]\n",
    "cat_corr_values = [x[1] for x in cat_mean_corrs]\n",
    "\n",
    "colors_grad = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(cat_names_sorted)))\n",
    "axes[1].barh(cat_names_sorted, cat_corr_values, color=colors_grad, edgecolor='black')\n",
    "axes[1].set_xlabel('Correla√ß√£o M√©dia com Target', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Import√¢ncia M√©dia por Categoria', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(cat_corr_values):\n",
    "    axes[1].text(v + 0.005, i, f'{v:.3f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. An√°lise de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar outliers usando IQR\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DE OUTLIERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outlier_counts = {}\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "    outlier_counts[col] = outliers\n",
    "\n",
    "outlier_df = pd.DataFrame.from_dict(outlier_counts, orient='index', columns=['Outliers'])\n",
    "outlier_df = outlier_df.sort_values('Outliers', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 features com mais outliers:\")\n",
    "print(outlier_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o de outliers\n",
    "top_features = correlations.head(6).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    data_by_class = [df[df['target'] == target_class][feature].values \n",
    "                     for target_class in df['target'].unique()]\n",
    "    \n",
    "    bp = axes[idx].boxplot(data_by_class, tick_labels=df['target'].unique(), \n",
    "                           patch_artist=True, showmeans=True, showfliers=True)\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    for flier, color in zip(bp['fliers'], colors):\n",
    "        flier.set(marker='o', color=color, markersize=4, alpha=0.5)\n",
    "    \n",
    "    short_name = feature.replace('mean_', '').replace('_', ' ')[:40]\n",
    "    axes[idx].set_title(f'{short_name}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Valor', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar contagem de outliers\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((df[feature] < lower_bound) | (df[feature] > upper_bound)).sum()\n",
    "    axes[idx].text(0.05, 0.95, f'Outliers: {outliers}', transform=axes[idx].transAxes,\n",
    "                   fontsize=9, verticalalignment='top', bbox=dict(boxstyle='round', \n",
    "                   facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Testes Estat√≠sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas por classe\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTAT√çSTICAS DESCRITIVAS POR CLASSE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "top_5_features = correlations.head(5).index.tolist()\n",
    "\n",
    "for feature in top_5_features:\n",
    "    print(f\"\\nFeature: {feature}\")\n",
    "    print(\"-\" * 80)\n",
    "    for target_class in df['target'].unique():\n",
    "        subset = df[df['target'] == target_class][feature]\n",
    "        print(f\"{target_class:10s}: mean={subset.mean():12.2f}, std={subset.std():12.2f}, \"\n",
    "              f\"min={subset.min():12.2f}, max={subset.max():12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testes T para as top 10 features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTES ESTAT√çSTICOS (T-TEST) - Top 10 Features\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n",
    "\n",
    "results = []\n",
    "for feature in correlations.head(10).index:\n",
    "    normal_data = df[df['target'] == 'normal'][feature]\n",
    "    interf_data = df[df['target'] == 'interf'][feature]\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(normal_data, interf_data)\n",
    "    results.append({\n",
    "        'Feature': feature.replace('mean_', '')[:40],\n",
    "        'T-Statistic': t_stat,\n",
    "        'P-Value': p_value,\n",
    "        'Significativo': 'Sim ‚úì' if p_value < 0.05 else 'N√£o ‚úó'\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumo e Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar resumo executivo\n",
    "print(\"=\" * 80)\n",
    "print(\"RESUMO EXECUTIVO DA AN√ÅLISE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä DATASET:\")\n",
    "print(f\"   ‚Ä¢ Total de amostras: {len(df)}\")\n",
    "print(f\"   ‚Ä¢ Total de features: {len(df.columns)-1}\")\n",
    "print(f\"   ‚Ä¢ Features num√©ricas: {len(numeric_cols)}\")\n",
    "\n",
    "print(f\"\\nüéØ CLASSES (TARGET):\")\n",
    "print(f\"   ‚Ä¢ 'interf': {(df['target']=='interf').sum()} amostras ({(df['target']=='interf').sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ 'normal': {(df['target']=='normal').sum()} amostras ({(df['target']=='normal').sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Desbalanceamento: {desbalanceamento:.1f}:1\")\n",
    "\n",
    "print(f\"\\n‚úÖ QUALIDADE DOS DADOS:\")\n",
    "print(f\"   ‚Ä¢ Valores ausentes: {df.isnull().sum().sum()}\")\n",
    "print(f\"   ‚Ä¢ Features com >80% zeros: {len(high_zeros)}\")\n",
    "print(f\"   ‚Ä¢ Duplicatas: {df.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nüîù TOP 5 FEATURES MAIS IMPORTANTES:\")\n",
    "for i, (feature, corr) in enumerate(correlations.head(5).items(), 1):\n",
    "    print(f\"   {i}. {feature.replace('mean_', '')[:40]:42s} (corr: {corr:.4f})\")\n",
    "\n",
    "print(f\"\\nüìÅ CATEGORIAS MAIS RELEVANTES:\")\n",
    "for i, (cat, corr) in enumerate(cat_mean_corrs[:3], 1):\n",
    "    print(f\"   {i}. {cat:20s} (corr m√©dia: {corr:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Principais Insights\n",
    "\n",
    "### üéØ Insights Principais\n",
    "\n",
    "1. **Desbalanceamento Severo:**\n",
    "   - 91.6% das amostras s√£o da classe 'interf'\n",
    "   - Necess√°rio usar t√©cnicas de balanceamento (SMOTE, class weights)\n",
    "\n",
    "2. **Feature Mais Importante:**\n",
    "   - `mean_os_net_num_connections` tem correla√ß√£o de 0.95 com o target\n",
    "   - Forte preditor da classe target\n",
    "\n",
    "3. **Categorias Relevantes:**\n",
    "   - M√©tricas de Network s√£o as mais correlacionadas\n",
    "   - Container Memory e Process Network tamb√©m s√£o relevantes\n",
    "\n",
    "4. **Caracter√≠sticas dos Dados:**\n",
    "   - 68 features com mais de 80% de zeros (sparsidade alta)\n",
    "   - Alta assimetria em v√°rias features\n",
    "   - Presen√ßa moderada de outliers\n",
    "\n",
    "### üí° Recomenda√ß√µes para Modelagem\n",
    "\n",
    "#### Pr√©-processamento:\n",
    "- Remover features com >90% de zeros\n",
    "- Aplicar normaliza√ß√£o/padroniza√ß√£o\n",
    "- Considerar transforma√ß√µes logar√≠tmicas para features assim√©tricas\n",
    "\n",
    "#### Balanceamento de Classes:\n",
    "- Usar SMOTE ou ADASYN para gerar amostras sint√©ticas\n",
    "- Aplicar class weights nos modelos\n",
    "- Considerar undersampling da classe majorit√°ria\n",
    "\n",
    "#### Feature Engineering:\n",
    "- Focar em m√©tricas de Network (maior poder preditivo)\n",
    "- Criar features agregadas por categoria\n",
    "- Aplicar sele√ß√£o de features (RFE, LASSO)\n",
    "\n",
    "#### Valida√ß√£o:\n",
    "- Usar Stratified K-Fold cross-validation\n",
    "- M√©tricas apropriadas: F1-score, AUC-ROC, Precision-Recall\n",
    "- Aten√ß√£o especial ao overfitting devido ao desbalanceamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com as principais estat√≠sticas\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Feature': correlations.head(20).index,\n",
    "    'Correlation': correlations.head(20).values,\n",
    "    'Mean': [df[col].mean() for col in correlations.head(20).index],\n",
    "    'Std': [df[col].std() for col in correlations.head(20).index],\n",
    "    'Zeros_Pct': [((df[col] == 0).sum() / len(df) * 100) for col in correlations.head(20).index]\n",
    "})\n",
    "\n",
    "print(\"\\nTop 20 Features - Estat√≠sticas Resumidas:\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar estat√≠sticas em CSV\n",
    "summary_stats.to_csv('eda_summary_statistics.csv', index=False)\n",
    "print(\"\\n‚úÖ Estat√≠sticas salvas em 'eda_summary_statistics.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ An√°lise Explorat√≥ria Conclu√≠da!\n",
    "\n",
    "Este notebook forneceu uma an√°lise completa do dataset, incluindo:\n",
    "- ‚úÖ An√°lise da vari√°vel target e desbalanceamento\n",
    "- ‚úÖ Identifica√ß√£o das features mais importantes\n",
    "- ‚úÖ An√°lise de correla√ß√µes e distribui√ß√µes\n",
    "- ‚úÖ Detec√ß√£o de outliers\n",
    "- ‚úÖ Testes estat√≠sticos\n",
    "- ‚úÖ Recomenda√ß√µes para modelagem\n",
    "\n",
    "**Pr√≥ximos passos:** Aplicar as recomenda√ß√µes de pr√©-processamento e desenvolver modelos de classifica√ß√£o (SVM, Random Forest, XGBoost, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
